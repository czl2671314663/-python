{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 使用LSTM进行情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们需要去创建词向量。为了简单起见，我们使用训练好的模型来创建。\n",
    "\n",
    "作为该领域的一个最大玩家，Google 已经帮助我们在大规模数据集上训练出来了 Word2Vec 模型，包括 1000 亿个不同的词！在这个模型中，谷歌能创建 300 万个词向量，每个向量维度为 300。\n",
    "\n",
    "在理想情况下，我们将使用这些向量来构建模型，但是因为这个单词向量矩阵相当大（3.6G），我们用另外一个现成的小一些的，该矩阵由 GloVe 进行训练得到。矩阵将包含 400000 个词向量，每个向量的维数为 50。\n",
    "\n",
    "我们将导入两个不同的数据结构，一个是包含 400000 个单词的 Python 列表，一个是包含所有单词向量值得 400000*50 维的嵌入矩阵。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#读取词数据集\n",
    "wordsList = np.load('D:/training_data/wordsList.npy')\n",
    "print('Loaded the word list!')#输出\n",
    "#已经训练好的词向量模型\n",
    "wordsList = wordsList.tolist() \n",
    "#给定相应格式\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] \n",
    "#读取词向量数据集\n",
    "wordVectors = np.load('D:/training_data/wordVectors.npy')\n",
    "print ('Loaded the word vectors!')#输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsList))#查看word list长度\n",
    "print(wordVectors.shape)#查看woedvectors的词向量模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以在词库中搜索单词，比如 “baseball”，然后可以通过访问嵌入矩阵来得到相应的向量，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.17981 , -0.40407 , -0.1653  , -0.60687 , -0.39656 ,  0.12688 ,\n",
       "       -0.053049,  0.38024 , -0.51008 ,  0.46593 , -0.30818 ,  0.79362 ,\n",
       "       -0.85766 , -0.25143 ,  1.0448  ,  0.18628 ,  0.13688 ,  0.092588,\n",
       "       -0.2236  , -0.13604 , -0.19482 ,  0.057702,  0.56133 ,  0.24823 ,\n",
       "        0.627   , -1.8437  , -1.2573  ,  0.64482 ,  1.2787  , -0.29522 ,\n",
       "        3.0493  ,  0.62079 ,  0.90369 , -0.030099, -0.13091 ,  0.30525 ,\n",
       "       -0.070138, -0.12912 ,  0.72277 , -0.79774 , -0.70277 ,  0.038009,\n",
       "        0.27192 ,  0.35679 ,  0.26493 ,  0.13037 , -0.01369 ,  0.33713 ,\n",
       "        0.99956 ,  0.72031 ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badIndex = wordsList.index('bad')#在word list中搜索baseball\n",
    "wordVectors[badIndex]#查看baseball这个词在词向量数据集中的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们有了向量，我们的第一步就是输入一个句子，然后构造它的向量表示。假设我们现在的输入句子是 “I thought the movie was incredible and inspiring”。为了得到词向量，我们可以使用 TensorFlow 的嵌入函数。这个函数有两个参数，一个是嵌入矩阵（在我们的情况下是词向量矩阵），另一个是每个词对应的索引。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[    41    804 201534   1005     15   7446      5  13767      0      0]\n"
     ]
    }
   ],
   "source": [
    "#可以设置文章的最大词数来限制\n",
    "maxSeqLength = 10 \n",
    "#每个单词的最大维度\n",
    "numDimensions = 300 \n",
    "firstSentence = np.zeros((maxSeqLength), dtype='int32')\n",
    "firstSentence[0] = wordsList.index(\"i\")\n",
    "firstSentence[1] = wordsList.index(\"thought\")\n",
    "firstSentence[2] = wordsList.index(\"the\")\n",
    "firstSentence[3] = wordsList.index(\"movie\")\n",
    "firstSentence[4] = wordsList.index(\"was\")\n",
    "firstSentence[5] = wordsList.index(\"incredible\")\n",
    "firstSentence[6] = wordsList.index(\"and\")\n",
    "firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "#如果长度没达到设置的标准，用0来占位\n",
    "print(firstSentence.shape)#查看类型\n",
    "#结果\n",
    "print(firstSentence) #查看结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据管道如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出数据是一个 10*50 的词矩阵，其中包括 10 个词，每个词的向量维度是 50。就是去找到这些词对应的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf#导入库\n",
    "#用tensorflow查看数组类型\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)#查看数组的类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在整个训练集上面构造索引之前，我们先花一些时间来可视化我们所拥有的数据类型。这将帮助我们去决定如何设置最大序列长度的最佳值。在前面的例子中，我们设置了最大长度为 10，但这个值在很大程度上取决于你输入的数据。\n",
    "\n",
    "训练集我们使用的是 IMDB 数据集。这个数据集包含 25000 条电影数据，其中 12500 条正向数据，12500 条负向数据。这些数据都是存储在一个文本文件中，首先我们需要做的就是去解析这个文件。正向数据包含在一个文件中，负向数据包含在另一个文件中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "情感积极数据集加载完毕\n",
      "情感消极数据集加载完毕\n",
      "总共文件数量 25000\n",
      "全部词语数量 5844680\n",
      "平均每篇评论词语数量 233.7872\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "#指定好数据集位置，由于提供的数据都一个个单独的文件，所以还得一个个读取\n",
    "#将三个文件夹的内容放到一个文件下面\n",
    "positiveFiles = ['D:/training_data/positiveReviews/' + f for f in listdir('D:/training_data/positiveReviews/') if isfile(join('D:/training_data/positiveReviews/', f))]\n",
    "negativeFiles = ['D:/training_data/negativeReviews/' + f for f in listdir('D:/training_data/negativeReviews/') if isfile(join('D:/training_data/negativeReviews/', f))]\n",
    "numWords = []#建一个空列表\n",
    "#分别统计积极和消极情感数据集\n",
    "#循环积极的数据集并添加到新类别里面\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('情感积极数据集加载完毕')\n",
    "#同理\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('情感消极数据集加载完毕')\n",
    "#输出类表查看\n",
    "numFiles = len(numWords)\n",
    "print('总共文件数量', numFiles)\n",
    "print('全部词语数量', sum(numWords))\n",
    "print('平均每篇评论词语数量', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#绘图查看，主要查看句子的长度\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)#间隔50\n",
    "plt.xlabel('Sequence Length')#x轴为长度\n",
    "plt.ylabel('Frequency')#y轴为次数\n",
    "plt.axis([0, 1200, 0, 8000])#设置图表的度量\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从直方图和句子的平均单词数，我们认为将句子最大长度设置为 250 是可行的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 250#设置筛选句子长度为250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，让我们看看如何将单个文件中的文本转换成索引矩阵，比如下面的代码就是文本中的其中一个评论。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, Titanic was a good movie, the first time you see it, but you really should see it a second time and your opinion of the film will definetly change. The first time you see the movie you see the underlying love-story and think: ooh, how romantic. The second time (and I am not the only one to think this) it is just annoying and you just sit there watching the movie thinking, When is this d**n ship going to sink??? And even this is not as impressive when you see it several times. The acting in this film is not bad, but definetly not great either. Was I glad DiCaprio did not win an oscar for that film, I mean who does he think he is, Anthony Hopkins or Denzel Washington? He does 1 half-good movie and won't do a film for less than $20 million. And then everyone is suprised that there are hardly any films with him in it. But enough about, in my eyes, the worst character of the film. Kate Winslet's performance on the other hand was wonderful. I also tink that the director is very talented to put a film of such a magnitude together. There is one lesson to be learned about this movie: there are too many love-stories as it is, filmmakers shouldn't try to add a crummy romance in to every single movie!!! Out of a possible 100% I give this film a mere 71%.\n"
     ]
    }
   ],
   "source": [
    "#随便哪一篇评论来看看结果\n",
    "fname = positiveFiles[30] \n",
    "with open(fname) as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n",
    "        exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将它转换成一个索引矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除标点符号、括号、问号等，只留下字母数字字符\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "#替换<br />为空值\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1085,  12608,     15,      7,    219,   1005, 201534,     58,\n",
       "           79,     81,    253,     20,     34,     81,    588,    189,\n",
       "          253,     20,      7,    126,     79,      5,    392,   2166,\n",
       "            3, 201534,    319,     43, 399999,    511, 201534,     58,\n",
       "           79,     81,    253, 201534,   1005,     81,    253, 201534,\n",
       "         6814, 399999,      5,    269,  50199,    197,   5845, 201534,\n",
       "          126,     79,      5,     41,    913,     36, 201534,     91,\n",
       "           48,      4,    269,     37,     20,     14,    120,  17249,\n",
       "            5,     81,    120,   3162,     63,   2641, 201534,   1005,\n",
       "         2412,     61,     14,     37,  69502,   1370,    222,      4,\n",
       "         9241,      5,    151,     37,     14,     36,     19,   4571,\n",
       "           61,     81,    253,     20,    201,    246, 201534,   2050,\n",
       "            6,     37,    319,     14,     36,    978,     34, 399999,\n",
       "           36,    353,    900,     15,     41,   7392,  26114,    119,\n",
       "           36,    320,     29,   3991,     10,     12,    319,     41,\n",
       "         1702,     38,    260,     18,    269,     18,     14,   3532,\n",
       "         7976,     46,  32921,    289,     18,    260,    176, 399999,\n",
       "         1005,      5,  58544,     88,      7,    319,     10,    440,\n",
       "           73,    324,     93,      5,    127,   1402,     14, 113848,\n",
       "           12,     63,     32,   4592,    130,   1588,     17,    103,\n",
       "            6,     20,     34,    575,     59,      6,    192,   2251,\n",
       "       201534,   1607,   1395,      3, 201534,    319,   7617, 399999,\n",
       "          883,     13, 201534,     68,    823,     15,   5205,     41,\n",
       "           52, 143320,     12, 201534,    369,     14,    191,   6918,\n",
       "            4,    339,      7,    319,      3,    125,      7,   6419,\n",
       "          600,     63,     14,     48,   6557,      4,     30,   2272,\n",
       "           59,     37,   1005,     63,     32,    317,    109, 399999,\n",
       "           19,     20,     14,  10363, 399999,    841,      4,   1679,\n",
       "            7,  66917,   7196,      6,      4,    359,    592,   1005,\n",
       "           66,      3,      7,    555,    558,     41,    455,     37,\n",
       "          319,      7,   6575,   5085,      0,      0,      0,      0,\n",
       "            0,      0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstFile = np.zeros((maxSeqLength), dtype='int32')#输入矩阵并循环出25000评论的词在上面那一段句子的个数，长度设为250，少的用0替代，多的删除\n",
    "#将每一个词放入矩阵中循环得出评分列表\n",
    "with open(fname) as f:\n",
    "    indexCounter = 0\n",
    "    line=f.readline()\n",
    "    cleanedLine = cleanSentences(line)\n",
    "    split = cleanedLine.split()\n",
    "    for word in split:\n",
    "        try:\n",
    "            firstFile[indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "        indexCounter = indexCounter + 1\n",
    "firstFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们用相同的方法来处理全部的 25000 条评论。我们将导入电影训练集，并且得到一个 25000 * 250 的矩阵。这是一个计算成本非常高的过程，可以直接使用理好的索引矩阵文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids = np.zeros((numFiles, maxSeqLength), dtype='int32')#定义词组的类型\n",
    "#fileCounter = 0\n",
    "##积极的\n",
    "#for pf in positiveFiles:\n",
    "#    with open(pf, \"r\",encoding='utf-8') as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1 \n",
    "##消极的\n",
    "#for nf in negativeFiles:\n",
    "#    with open(nf, \"r\" ,encoding='utf-8') as f:\n",
    "#        indexCounter = 0\n",
    "#        line=f.readline()\n",
    "#        cleanedLine = cleanSentences(line)\n",
    "#        split = cleanedLine.split()\n",
    "#        for word in split:\n",
    "#            try:\n",
    "#                ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#            except ValueError:\n",
    "#                ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#            indexCounter = indexCounter + 1\n",
    "#            if indexCounter >= maxSeqLength:\n",
    "#                break\n",
    "#        fileCounter = fileCounter + 1 \n",
    "# #Pass into embedding function and see if it evaluates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('idsMatrix2.0', ids)#保存处理后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.load('C:/Users/czl/大三小学期/第一周/idsMatrix2.0.npy')#导入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "# 制作batch数据，通过数据集索引位置来设置训练集和测试集\n",
    "#并且让batch中正负样本各占一半，同时给定其当前标签\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,11499)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(13499,24999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(11499,13499)\n",
    "        if (num <= 12499):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以开始构建我们的 TensorFlow 图模型。首先，我们需要去定义一些超参数，比如批处理大小，LSTM的单元个数，分类类别和训练次数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 24#批处理大小\n",
    "lstmUnits = 64#LSTM的单元个数\n",
    "numClasses = 2#分类类别\n",
    "iterations = 50000#训练次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anoconda\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()#将tensorflow2.0换成1.0\n",
    "\n",
    "tf.reset_default_graph()#清除默认图形堆栈并重置全局默认图形\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])#函数参数，在执行的时候再赋具体的值\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦，我们设置了我们的输入数据占位符，我们可以调用\n",
    "tf.nn.embedding_lookup() 函数来得到我们的词向量。该函数最后将返回一个三维向量，第一个维度是批处理大小，第二个维度是句子长度，第三个维度是词向量长度。更清晰的表达，如下图所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)#该操作返回一个带有形状shape的类型为dtype张量,并且所有元素都设为零.\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)#把每个词都映射成向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经得到了我们想要的数据形式，那么揭晓了我们看看如何才能将这种数据形式输入到我们的 LSTM 网络中。首先，我们使用 tf.nn.rnn_cell.BasicLSTMCell 函数，这个函数输入的参数是一个整数，表示需要几个 LSTM 单元。这是我们设置的一个超参数，我们需要对这个数值进行调试从而来找到最优的解。然后，我们会设置一个 dropout 参数，以此来避免一些过拟合。\n",
    "\n",
    "最后，我们将 LSTM cell 和三维的数据输入到 tf.nn.dynamic_rnn ，这个函数的功能是展开整个网络，并且构建一整个 RNN 模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-0c3ebf9db47d>:9: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From D:\\anoconda\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:437: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From D:\\anoconda\\lib\\site-packages\\keras\\layers\\rnn\\legacy_cells.py:958: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-0c3ebf9db47d>:5: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(lstmUnits, state_is_tuple=True),output_keep_prob=0.5)\n",
      "<ipython-input-19-0c3ebf9db47d>:7: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(lstmUnits, state_is_tuple=True),output_keep_prob=0.5)\n"
     ]
    }
   ],
   "source": [
    "#lstmCell = tf.nn.rnn_cell.BasicLSTMCell(lstmUnits)\n",
    "#lstmCell = tf.nn.rnn_cell.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "#value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32) \n",
    "# 定义正向LSTM结构\n",
    "lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(lstmUnits, state_is_tuple=True),output_keep_prob=0.5)   \n",
    "# 定义反向LSTM结构\n",
    "lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(lstmUnits, state_is_tuple=True),output_keep_prob=0.5)\n",
    "# 将双向结构结合起来\n",
    "value,_ = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell,data, dtype=tf.float32,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "堆栈 LSTM 网络是一个比较好的网络架构。也就是前一个LSTM 隐藏层的输出是下一个LSTM的输入。堆栈LSTM可以帮助模型记住更多的上下文信息，但是带来的弊端是训练参数会增加很多，模型的训练时间会很长，过拟合的几率也会增加。\n",
    "\n",
    "dynamic RNN 函数的第一个输出可以被认为是最后的隐藏状态向量。这个向量将被重新确定维度，然后乘以最后的权重矩阵和一个偏置项来获得最终的输出值。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##权重参数初始化\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value[0],[1,0,2])\n",
    "##取最终的结果值\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们需要定义正确的预测函数和正确率评估参数。正确的预测形式是查看最后输出的0-1向量是否和标记的0-1向量相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后，我们使用一个标准的交叉熵损失函数来作为损失值。对于优化器，我们选择 Adam，并且采用默认的学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anoconda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#用交叉熵损失函数来作为损失值\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "#选择Adam优化器，并且采用默认的学习率\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在tensorboard中绘图\n",
    "tf.summary.scalar('acc',accuracy)\n",
    "tf.summary.scalar('loss',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超参数调整"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练过程的基本思路是，我们首先先定义一个 TensorFlow 会话。然后，我们加载一批评论和对应的标签。接下来，我们调用会话的 run 函数。这个函数有两个参数，第一个参数被称为 fetches 参数，这个参数定义了我们感兴趣的值。我们希望通过我们的优化器来最小化损失函数。第二个参数被称为 feed_dict 参数。这个数据结构就是我们提供给我们的占位符。我们需要将一个批处理的评论和标签输入模型，然后不断对这一组训练数据进行循环训练。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1001/50000... loss 0.48133882880210876... accuracy 0.75...\n",
      "iteration 2001/50000... loss 0.5578255653381348... accuracy 0.6666666865348816...\n",
      "iteration 3001/50000... loss 0.4463171064853668... accuracy 0.7916666865348816...\n",
      "iteration 4001/50000... loss 0.560309886932373... accuracy 0.6666666865348816...\n",
      "iteration 5001/50000... loss 0.6010549664497375... accuracy 0.625...\n",
      "iteration 6001/50000... loss 0.5225700736045837... accuracy 0.7083333134651184...\n",
      "iteration 7001/50000... loss 0.6023810505867004... accuracy 0.625...\n",
      "iteration 8001/50000... loss 0.6372889876365662... accuracy 0.5833333134651184...\n",
      "iteration 9001/50000... loss 0.3969896733760834... accuracy 0.8333333134651184...\n",
      "iteration 10001/50000... loss 0.5565477013587952... accuracy 0.6666666865348816...\n",
      "INFO:tensorflow:models\\pretrained_lstm.ckpt-10000.data-00000-of-00001\n",
      "INFO:tensorflow:7700\n",
      "INFO:tensorflow:models\\pretrained_lstm.ckpt-10000.index\n",
      "INFO:tensorflow:7700\n",
      "INFO:tensorflow:models\\pretrained_lstm.ckpt-10000.meta\n",
      "INFO:tensorflow:87900\n",
      "saved to models/pretrained_lstm.ckpt-10000\n",
      "iteration 11001/50000... loss 0.579529345035553... accuracy 0.6666666865348816...\n",
      "iteration 12001/50000... loss 0.47691741585731506... accuracy 0.75...\n",
      "iteration 13001/50000... loss 0.4885749816894531... accuracy 0.75...\n",
      "iteration 14001/50000... loss 0.44048011302948... accuracy 0.7916666865348816...\n",
      "iteration 15001/50000... loss 0.6120132803916931... accuracy 0.625...\n",
      "iteration 16001/50000... loss 0.40156689286231995... accuracy 0.8333333134651184...\n",
      "iteration 17001/50000... loss 0.5264316201210022... accuracy 0.7083333134651184...\n",
      "iteration 18001/50000... loss 0.5207344889640808... accuracy 0.7083333134651184...\n",
      "iteration 19001/50000... loss 0.5612459778785706... accuracy 0.6666666865348816...\n",
      "iteration 20001/50000... loss 0.4818519651889801... accuracy 0.75...\n",
      "INFO:tensorflow:models\\pretrained_lstm.ckpt-20000.data-00000-of-00001\n",
      "INFO:tensorflow:7700\n",
      "INFO:tensorflow:models\\pretrained_lstm.ckpt-20000.index\n",
      "INFO:tensorflow:7700\n",
      "INFO:tensorflow:models\\pretrained_lstm.ckpt-20000.meta\n",
      "INFO:tensorflow:87900\n",
      "saved to models/pretrained_lstm.ckpt-20000\n",
      "iteration 21001/50000... loss 0.6574813723564148... accuracy 0.5833333134651184...\n",
      "iteration 22001/50000... loss 0.6077480912208557... accuracy 0.625...\n",
      "iteration 23001/50000... loss 0.6479288935661316... accuracy 0.5833333134651184...\n",
      "iteration 24001/50000... loss 0.6060387492179871... accuracy 0.625...\n",
      "iteration 25001/50000... loss 0.6038098931312561... accuracy 0.625...\n",
      "iteration 26001/50000... loss 0.48390093445777893... accuracy 0.75...\n",
      "iteration 27001/50000... loss 0.6046281456947327... accuracy 0.625...\n",
      "iteration 28001/50000... loss 0.47939029335975647... accuracy 0.75...\n",
      "iteration 29001/50000... loss 0.52354896068573... accuracy 0.7083333134651184...\n",
      "iteration 30001/50000... loss 0.5982151627540588... accuracy 0.625...\n",
      "INFO:tensorflow:models\\pretrained_lstm.ckpt-30000.data-00000-of-00001\n",
      "INFO:tensorflow:7700\n",
      "INFO:tensorflow:models\\pretrained_lstm.ckpt-30000.index\n",
      "INFO:tensorflow:7700\n",
      "INFO:tensorflow:models\\pretrained_lstm.ckpt-30000.meta\n",
      "INFO:tensorflow:87900\n",
      "saved to models/pretrained_lstm.ckpt-30000\n",
      "iteration 31001/50000... loss 0.4385727345943451... accuracy 0.7916666865348816...\n",
      "iteration 32001/50000... loss 0.47912225127220154... accuracy 0.75...\n",
      "iteration 33001/50000... loss 0.5601025223731995... accuracy 0.6666666865348816...\n",
      "iteration 34001/50000... loss 0.5196780562400818... accuracy 0.7083333134651184...\n",
      "iteration 35001/50000... loss 0.6407454609870911... accuracy 0.5833333134651184...\n",
      "iteration 36001/50000... loss 0.5653879046440125... accuracy 0.6666666865348816...\n",
      "iteration 37001/50000... loss 0.5576604008674622... accuracy 0.6666666865348816...\n",
      "iteration 38001/50000... loss 0.43598830699920654... accuracy 0.7916666865348816...\n",
      "iteration 39001/50000... loss 0.5674257874488831... accuracy 0.6666666865348816...\n",
      "iteration 40001/50000... loss 0.596138060092926... accuracy 0.625...\n",
      "INFO:tensorflow:models\\pretrained_lstm.ckpt-40000.data-00000-of-00001\n",
      "INFO:tensorflow:7700\n",
      "INFO:tensorflow:models\\pretrained_lstm.ckpt-40000.index\n",
      "INFO:tensorflow:7700\n",
      "INFO:tensorflow:models\\pretrained_lstm.ckpt-40000.meta\n",
      "INFO:tensorflow:87900\n",
      "saved to models/pretrained_lstm.ckpt-40000\n",
      "iteration 41001/50000... loss 0.6017020344734192... accuracy 0.625...\n",
      "iteration 42001/50000... loss 0.523870050907135... accuracy 0.7083333134651184...\n",
      "iteration 43001/50000... loss 0.48311153054237366... accuracy 0.75...\n",
      "iteration 44001/50000... loss 0.5604053139686584... accuracy 0.6666666865348816...\n",
      "iteration 45001/50000... loss 0.5236547589302063... accuracy 0.7083333134651184...\n",
      "iteration 46001/50000... loss 0.5626168847084045... accuracy 0.6666666865348816...\n",
      "iteration 47001/50000... loss 0.5185345411300659... accuracy 0.7083333134651184...\n",
      "iteration 48001/50000... loss 0.39963439106941223... accuracy 0.8333333134651184...\n",
      "iteration 49001/50000... loss 0.439349889755249... accuracy 0.7916666865348816...\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "saver = tf.train.Saver()\n",
    "#循环保存信息\n",
    "with tf.Session() as sess:\n",
    "    merge_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"D:/data/logs/\", sess.graph) #保存神经网络的所有的信息，方便浏览器访问\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "\n",
    "    \n",
    "    for i in range(iterations):\n",
    "    #之前已经定义好的拿到batch数据函数\n",
    "        nextBatch, nextBatchLabels = getTrainBatch();\n",
    "        sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        \n",
    "        \n",
    "        \n",
    "        #每隔1000次打印一下当前的结果\n",
    "        if (i % 1000 == 0 and i != 0):\n",
    "            loss_ = sess.run(loss, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "            accuracy_ = sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "\n",
    "            result = sess.run(merge_summary, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "            writer.add_summary(result, i)\n",
    "\n",
    "            print(\"iteration {}/{}...\".format(i+1, iterations),\n",
    "                  \"loss {}...\".format(loss_),\n",
    "                  \"accuracy {}...\".format(accuracy_))    \n",
    "      #每个1W次保存一下当前模型\n",
    "        if (i % 10000 == 0 and i != 0):\n",
    "            save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "            print(\"saved to %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/SentimentAnalysis6.png)\n",
    "![caption](Images/SentimentAnalysis7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看上面的训练曲线，我们发现这个模型的训练结果还是不错的。损失值在稳定的下降，正确率也不断的在接近 100% 。然而，当分析训练曲线的时候，我们应该注意到我们的模型可能在训练集上面已经过拟合了。过拟合是机器学习中一个非常常见的问题，表示模型在训练集上面拟合的太好了，但是在测试集上面的泛化能力就会差很多。也就是说，如果你在训练集上面取得了损失值是 0 的模型，但是这个结果也不一定是最好的结果。当我们训练 LSTM 的时候，提前终止是一种常见的防止过拟合的方法。基本思路是，我们在训练集上面进行模型训练，同事不断的在测试集上面测量它的性能。一旦测试误差停止下降了，或者误差开始增大了，那么我们就需要停止训练了。因为这个迹象表明，我们网络的性能开始退化了。\n",
    "\n",
    "导入一个预训练的模型需要使用 TensorFlow 的另一个会话函数，称为 Server ，然后利用这个会话函数来调用 restore 函数。这个函数包括两个参数，一个表示当前的会话，另一个表示保存的模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models\\pretrained_lstm.ckpt-40000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()#使用这个会话可以得到张量的结果\n",
    "saver = tf.train.Saver()#保存加载模型\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))#checkpoints文件保存和从checkpoints文件中恢复变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，从我们的测试集中导入一些电影评论。请注意，这些评论是模型从来没有看见过的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 66.66666865348816\n",
      "Accuracy for this batch: 58.33333134651184\n",
      "Accuracy for this batch: 54.16666865348816\n",
      "Accuracy for this batch: 33.33333432674408\n",
      "Accuracy for this batch: 66.66666865348816\n",
      "Accuracy for this batch: 66.66666865348816\n",
      "Accuracy for this batch: 41.66666567325592\n",
      "Accuracy for this batch: 75.0\n",
      "Accuracy for this batch: 58.33333134651184\n"
     ]
    }
   ],
   "source": [
    "iterations = 10#导入10条\n",
    "#循环查看精确率\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
